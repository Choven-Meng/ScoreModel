{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = pd.read_csv('C:\\\\Users\\\\choven\\\\Desktop\\\\scoreCard\\\\J03ScoreCard\\\\A_Card\\\\application.csv',encoding='latin1') #Latin1是ISO-8859-1的别名,单字节编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39785, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>desc</th>\n",
       "      <th>...</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Borrower added on 12/22/11 &gt; I need to upgra...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec-11</td>\n",
       "      <td>Jan-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2500</td>\n",
       "      <td>60 months</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Borrower added on 12/22/11 &gt; I plan to use t...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec-11</td>\n",
       "      <td>Apr-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec-11</td>\n",
       "      <td>Nov-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   member_id  loan_amnt        term  loan_status int_rate emp_length  \\\n",
       "0          1       5000   36 months   Fully Paid   10.65%  10+ years   \n",
       "1          2       2500   60 months  Charged Off   15.27%   < 1 year   \n",
       "2          3       2400   36 months   Fully Paid   15.96%  10+ years   \n",
       "\n",
       "  home_ownership  annual_inc verification_status  \\\n",
       "0           RENT     24000.0            Verified   \n",
       "1           RENT     30000.0     Source Verified   \n",
       "2           RENT     12252.0        Not Verified   \n",
       "\n",
       "                                                desc        ...         \\\n",
       "0    Borrower added on 12/22/11 > I need to upgra...        ...          \n",
       "1    Borrower added on 12/22/11 > I plan to use t...        ...          \n",
       "2                                                NaN        ...          \n",
       "\n",
       "  delinq_2yrs inq_last_6mths mths_since_last_delinq mths_since_last_record  \\\n",
       "0           0              1                    NaN                    NaN   \n",
       "1           0              5                    NaN                    NaN   \n",
       "2           0              2                    NaN                    NaN   \n",
       "\n",
       "   open_acc  pub_rec  total_acc  pub_rec_bankruptcies  issue_d  \\\n",
       "0         3        0          9                   0.0   Dec-11   \n",
       "1         3        0          4                   0.0   Dec-11   \n",
       "2         2        0         10                   0.0   Dec-11   \n",
       "\n",
       "   earliest_cr_line  \n",
       "0            Jan-85  \n",
       "1            Apr-99  \n",
       "2            Nov-01  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉 month，只保留数字\n",
    "dataSet['term'] = dataSet['term'].apply(lambda x: int(x.replace(' months', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理标签：Fully Paid是正常用户；Charged Off是违约用户\n",
    "# (charged off:1,fully paid:0)\n",
    "dataSet['y'] = dataSet['loan_status'].map(lambda x: int(x == 'Charged Off')) # int(True) == 1, int(Fales) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "由于存在不同的贷款期限（term），申请评分卡模型评估的违约概率必须要在统一的期限中，且不宜太长，所以选取term＝36months的行本\n",
    "'''\n",
    "\n",
    "dataCopy = dataSet.loc[dataSet.term == 36]\n",
    "\n",
    "trainData, testData = train_test_split(dataCopy, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29095, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCopy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17457, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11638, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固化变量\n",
    "trainDataFile = open(folderOfData + 'trainData.pkl', 'wb+')\n",
    "pickle.dump(trainData, trainDataFile)\n",
    "trainDataFile.close()\n",
    "\n",
    "testDataFile = open(folderOfData + 'testData.pkl', 'wb+')\n",
    "pickle.dump(testData, testDataFile)\n",
    "testDataFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：数据预处理\n",
    "\n",
    "1. 数据清洗\n",
    "\n",
    "2. 格式转换\n",
    "\n",
    "3. 确实值填补\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将带％的百分比变为浮点数\n",
    "trainData['int_rate_clean'] = trainData['int_rate'].map(lambda x: float(x.replace('%', '')) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10+ years    3509\n",
       "< 1 year     2189\n",
       "2 years      2018\n",
       "3 years      1824\n",
       "1 year       1569\n",
       "4 years      1561\n",
       "5 years      1431\n",
       "6 years       956\n",
       "7 years       727\n",
       "8 years       641\n",
       "9 years       555\n",
       "Name: emp_length, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.emp_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将工作年限进行转化，否则影响排序\n",
    "import re\n",
    "def CareerYear(x):\n",
    "    #对工作年限进行转换\n",
    "    x = str(x)\n",
    "    # str.find(subsStr)  # 如果包含子字符串返回开始的索引值，否则返回-1。\n",
    "    if x.find('nan') > -1:\n",
    "        return -1\n",
    "    elif x.find(\"10+\")>-1:   #将\"10＋years\"转换成 11\n",
    "        return 11\n",
    "    elif x.find('< 1') > -1:  #将\"< 1 year\"转换成 0\n",
    "        return 0\n",
    "    else:\n",
    "        return int(re.sub(\"\\D\", \"\", x))   #其余数据，去掉\"years\"并转换成整数\n",
    "    \n",
    "    \n",
    "trainData['emp_length_clean'] = trainData['emp_length'].map(CareerYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将desc的缺失作为一种状态，非缺失作为另一种状态\n",
    "def DescExisting(x):\n",
    "    #将desc变量转换成有记录和无记录两种\n",
    "    # type(np.nan) == float\n",
    "    if type(x).__name__ == 'float':\n",
    "        return 'no desc'\n",
    "    else:\n",
    "        return 'desc'\n",
    "    \n",
    "    \n",
    "trainData['desc_clean'] = trainData['desc'].map(DescExisting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理日期。earliest_cr_line的格式不统一，需要统一格式且转换成python的日期\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "def ConvertDateStr(x):\n",
    "    mth_dict = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10,\n",
    "                'Nov': 11, 'Dec': 12}\n",
    "    if str(x) == 'nan':   # str(np.nan) == 'nan'\n",
    "        # time.mktime() 返回用秒数来表示时间的浮点数 (字符串时间转时间戳)\n",
    "        # datetime.fromtimestamp（） 时间戳转datatime对象\n",
    "        return datetime.datetime.fromtimestamp(time.mktime(time.strptime('3000-1','%Y-%m')))\n",
    "        #time.mktime 不能读取1970年之前的日期\n",
    "    else:\n",
    "        yr = int(x[4:6])  # str[4:6]  选取字符串[4:6]位置的数\n",
    "        if yr <=17:\n",
    "            yr = 2000+yr\n",
    "        else:\n",
    "            yr = 1900 + yr\n",
    "        mth = mth_dict[x[:3]]\n",
    "        return datetime.datetime(yr,mth,1)\n",
    "    \n",
    "    \n",
    "trainData['app_date_clean'] = trainData['issue_d'].map(lambda x: ConvertDateStr(x))\n",
    "trainData['earliest_cr_line_clean'] = trainData['earliest_cr_line'].map(lambda x: ConvertDateStr(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理mths_since_last_delinq。注意原始值中有空值，所以用－1代替缺失\n",
    "\n",
    "def MakeupMissing(x):\n",
    "    if np.isnan(x):\n",
    "        return -1\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "trainData['mths_since_last_delinq_clean'] = trainData['mths_since_last_delinq'].map(lambda x: MakeupMissing(x))\n",
    "\n",
    "trainData['mths_since_last_record_clean'] = trainData['mths_since_last_record'].map(lambda x: MakeupMissing(x))\n",
    "\n",
    "trainData['pub_rec_bankruptcies_clean'] = trainData['pub_rec_bankruptcies'].map(lambda x: MakeupMissing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 第二步：变量衍生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 考虑申请额度与收入的占比\n",
    "trainData['limit_income'] = trainData.apply(lambda x: x.loan_amnt / x.annual_inc, axis = 1)\n",
    "\n",
    "# 考虑earliest_cr_line到申请日期的跨度，以月份记\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def MonthGap(earlyDate, lateDate):\n",
    "    if lateDate > earlyDate:\n",
    "        gap = relativedelta(lateDate,earlyDate)\n",
    "        \n",
    "        yr = gap.years\n",
    "        mth = gap.months\n",
    "        return yr*12+mth\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "trainData['earliest_cr_to_app'] = trainData.apply(lambda x: MonthGap(x.earliest_cr_line_clean,x.app_date_clean), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：分箱，采用ChiMerge,要求分箱完之后：\n",
    "\n",
    "1. 不超过5箱\n",
    "\n",
    "2. Bad Rate单调\n",
    "\n",
    "3. 每箱同时包含好坏样本\n",
    "\n",
    "4. 特殊值如－1，单独成一箱\n",
    "\n",
    "连续型变量可直接分箱\n",
    "\n",
    "类别型变量：\n",
    "\n",
    "1. 当取值较多时，先用bad rate编码，再用连续型分箱的方式进行分箱\n",
    "\n",
    "2. 当取值较少时：\n",
    "\n",
    "    （1）如果每种类别同时包含好坏样本，无需分箱\n",
    "    \n",
    "    （2）如果有类别只包含好坏样本的一种，需要合并\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['int_rate_clean', 'emp_length_clean', 'annual_inc', 'dti', 'delinq_2yrs', 'earliest_cr_to_app',\n",
    "                'inq_last_6mths', \n",
    "                'mths_since_last_record_clean', 'mths_since_last_delinq_clean', 'open_acc', 'pub_rec', 'total_acc',\n",
    "                'limit_income', 'earliest_cr_to_app']\n",
    "\n",
    "cat_features = ['home_ownership', 'verification_status', 'desc_clean', 'purpose', 'zip_code', 'addr_state',\n",
    "                'pub_rec_bankruptcies_clean']\n",
    "\n",
    "more_value_features = []  # 类别标签中unique值 > 5\n",
    "less_value_features = []  # 类别标签中unique值 < 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步，检查类别型变量中，哪些变量取值超过5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in cat_features:\n",
    "    valueCounts = len(set(trainData[var]))\n",
    "    #print(valueCounts)\n",
    "    if valueCounts > 5:\n",
    "        more_value_features.append(var)  # 取值超过5的变量，需要bad rate编码，再用卡方分箱法进行分箱\n",
    "    else:\n",
    "        less_value_features.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "离散型标签中unique>5的more_value_features：\n",
      "     ['purpose', 'zip_code', 'addr_state']\n",
      "---------------------\n",
      "离散型标签中unique<5的less_value_features：\n",
      "     ['verification_status', 'desc_clean']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "\"\"\"\n",
    "\n",
    "print(\"离散型标签中unique>5的more_value_features：\")\n",
    "print(\"    \",more_value_features)\n",
    "print('---------------------')\n",
    "print(\"离散型标签中unique<5的less_value_features：\")\n",
    "print(\"    \",less_value_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinBadRate(df, col, target, grantRateIndicator=0):\n",
    "    '''\n",
    "    :param df: 需要计算好坏比率的数据集\n",
    "    :param col: 需要计算好坏比率的特征\n",
    "    :param target: 好坏标签\n",
    "    :param grantRateIndicator: 1返回总体的坏样本率，0不返回\n",
    "    :return: 每箱的坏样本率，以及总体的坏样本率（当grantRateIndicator＝＝1时）\n",
    "    '''\n",
    "    total = df.groupby([col])[target].count()\n",
    "    total = pd.DataFrame({'total': total})\n",
    "    bad = df.groupby([col])[target].sum()\n",
    "    bad = pd.DataFrame({'bad': bad})\n",
    "    regroup = total.merge(bad, left_index=True, right_index=True, how='left') # 每箱的坏样本数，总样本数\n",
    "    regroup.reset_index(level=0, inplace=True)  # 更改索引\n",
    "    regroup['bad_rate'] = regroup.apply(lambda x: x.bad * 1.0 / x.total, axis=1) # 加上一列坏样本率\n",
    "    dicts = dict(zip(regroup[col],regroup['bad_rate'])) # col箱对应的坏样本率组成的字典\n",
    "    if grantRateIndicator==0:\n",
    "        return (dicts,regroup)\n",
    "    N = sum(regroup['total'])\n",
    "    B = sum(regroup['bad'])\n",
    "    overallRate = B * 1.0 / N\n",
    "    return (dicts, regroup, overallRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_rec_bankruptcies_clean</th>\n",
       "      <th>total</th>\n",
       "      <th>bad</th>\n",
       "      <th>bad_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16385</td>\n",
       "      <td>1733</td>\n",
       "      <td>0.105767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>673</td>\n",
       "      <td>101</td>\n",
       "      <td>0.150074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>397</td>\n",
       "      <td>67</td>\n",
       "      <td>0.168766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pub_rec_bankruptcies_clean  total   bad  bad_rate\n",
       "0                         0.0  16385  1733  0.105767\n",
       "1                         1.0    673   101  0.150074\n",
       "2                        -1.0    397    67  0.168766\n",
       "3                         2.0      2     2  1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test \n",
    "# BinBadRate\n",
    "\"\"\"\n",
    "regroup = BinBadRate(trainData,'pub_rec_bankruptcies_clean','y')[1].sort_values(by = 'bad_rate')\n",
    "regroup.index = range(regroup.shape[0])\n",
    "regroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （i）当取值<5时：如果每种类别同时包含好坏样本，无需分箱；如果有类别只包含好坏样本的一种，需要合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeBad0(df,col,target, direction='bad'):\n",
    "    '''\n",
    "     :param df: 包含检验0％或者100%坏样本率\n",
    "     :param col: 分箱后的变量或者类别型变量。检验其中是否有一组或者多组没有坏样本或者没有好样本。如果是，则需要进行合并\n",
    "     :param target: 目标变量，0、1表示好、坏\n",
    "     :return: 合并方案，使得每个组里同时包含好坏样本\n",
    "     '''\n",
    "    regroup = BinBadRate(df, col, target)[1]\n",
    "    if direction == 'bad':\n",
    "        # 如果是合并0坏样本率的组，则跟最小的非0坏样本率的组进行合并\n",
    "        regroup = regroup.sort_values(by = 'bad_rate')\n",
    "    else:\n",
    "        # 如果是合并0好样本样本率的组，则跟最小的非0好样本率的组进行合并\n",
    "        regroup = regroup.sort_values(by='bad_rate',ascending=False)\n",
    "        \n",
    "    regroup.index = range(regroup.shape[0]) # 重置regroup索引，使其从0开始\n",
    "    col_regroup = [[i] for i in regroup[col]] # col列的unique值\n",
    "    del_index = []\n",
    "    for i in range(regroup.shape[0]-1):\n",
    "        col_regroup[i+1] = col_regroup[i] + col_regroup[i+1]\n",
    "        del_index.append(i)  # 需要合并的变量索引集合\n",
    "        if direction == 'bad':\n",
    "            if regroup['bad_rate'][i+1] > 0:\n",
    "                break\n",
    "        else:\n",
    "            if regroup['bad_rate'][i+1] < 1:\n",
    "                break\n",
    "    col_regroup2 = [col_regroup[i] for i in range(len(col_regroup)) if i not in del_index] # 不需合并的变量集合\n",
    "    newGroup = {}\n",
    "    for i in range(len(col_regroup2)):\n",
    "        for g2 in col_regroup2[i]:\n",
    "            newGroup[g2] = 'Bin '+str(i)\n",
    "    return newGroup # 需要合并的变量具有相同的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_regroup:  [[0.0], [0.0, 1.0], [-1.0], [2.0]]\n",
      "---------------------\n",
      "需要合并的del_index:  [0]\n",
      "---------------------\n",
      "不需要合并的col_regroup2:  [[0.0, 1.0], [-1.0], [2.0]]\n",
      "---------------------\n",
      "合并后结果:  {0.0: 'Bin 0', 1.0: 'Bin 0', 2.0: 'Bin 2', -1.0: 'Bin 1'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "# 离散型unique <5 的col变量合并，使其变量中同时包含好坏\n",
    "\"\"\"\n",
    "direction = 'bad'\n",
    "col = 'pub_rec_bankruptcies_clean'\n",
    "col_regroup = [[i] for i in regroup[col]] # col列的unique值\n",
    "del_index = []\n",
    "for i in range(regroup.shape[0]-1):\n",
    "    col_regroup[i+1] = col_regroup[i] + col_regroup[i+1]\n",
    "    del_index.append(i)\n",
    "    if direction == 'bad':\n",
    "        if regroup['bad_rate'][i+1] > 0:\n",
    "            break\n",
    "    else:\n",
    "        if regroup['bad_rate'][i+1] < 1:\n",
    "            break\n",
    "col_regroup2 = [col_regroup[i] for i in range(len(col_regroup)) if i not in del_index] \n",
    "\n",
    "\n",
    "print(\"col_regroup: \",col_regroup)\n",
    "print('---------------------')\n",
    "print(\"需要合并的del_index: \",del_index)\n",
    "print('---------------------')\n",
    "print(\"不需要合并的col_regroup2: \",col_regroup2)\n",
    "print('---------------------')\n",
    "print(\"合并后结果: \",MergeBad0(trainData,col,'y', direction='bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_ownership need to be combined due to 0 bad rate\n",
      "pub_rec_bankruptcies_clean need to be combined due to 0 good rate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merge_bin_dict = {}  # 存放需要合并的变量，以及合并方法(即改编后相同的值合并)\n",
    "var_bin_list = []  # 由于某个取值没有好或者坏样本而需要合并的变量\n",
    "for col in less_value_features:\n",
    "    binBadRate = BinBadRate(trainData, col, 'y')[0]\n",
    "    if min(binBadRate.values()) == 0:  # 由于某个取值没有坏样本而进行合并\n",
    "        print('{} need to be combined due to 0 bad rate'.format(col))\n",
    "        combine_bin = MergeBad0(trainData, col, 'y')\n",
    "        merge_bin_dict[col] = combine_bin\n",
    "        newVar = col + '_Bin'\n",
    "        trainData[newVar] = trainData[col].map(combine_bin)\n",
    "        var_bin_list.append(newVar)\n",
    "    if max(binBadRate.values()) == 1:  # 由于某个取值没有好样本而进行合并\n",
    "        print('{} need to be combined due to 0 good rate'.format(col))\n",
    "        combine_bin = MergeBad0(trainData, col, 'y', direction='good')\n",
    "        merge_bin_dict[col] = combine_bin\n",
    "        newVar = col + '_Bin'\n",
    "        trainData[newVar] = trainData[col].map(combine_bin)\n",
    "        var_bin_list.append(newVar)\n",
    "        \n",
    "# less_value_features里剩下不需要合并的变量\n",
    "less_value_features = [i for i in less_value_features if i + '_Bin' not in var_bin_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由于变量不同时包含好坏需要合并的变量：\n",
      "     ['home_ownership_Bin', 'pub_rec_bankruptcies_clean_Bin']\n",
      "---------------------\n",
      "unique<5的特征中不需要合并的变量\n",
      "     ['verification_status', 'desc_clean']\n",
      "---------------------\n",
      "需要合并的col及方法： \n",
      "     {'home_ownership': {'RENT': 'Bin 2', 'NONE': 'Bin 0', 'OTHER': 'Bin 3', 'MORTGAGE': 'Bin 0', 'OWN': 'Bin 1'}, 'pub_rec_bankruptcies_clean': {0.0: 'Bin 2', 1.0: 'Bin 1', 2.0: 'Bin 0', -1.0: 'Bin 0'}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "# unique<5 的需要合并变量的离散型特征结果\n",
    "\"\"\"\n",
    "print(\"由于变量不同时包含好坏需要合并的变量：\")\n",
    "print(\"    \",var_bin_list)\n",
    "print('---------------------')\n",
    "print('unique<5的特征中不需要合并的变量')\n",
    "print(\"    \",less_value_features)\n",
    "print('---------------------')\n",
    "print(\"需要合并的col及方法： \")\n",
    "print(\"    \",merge_bin_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存merge_bin_dict\n",
    "file1 = open(folderOfData + 'merge_bin_dict.pkl', 'wb+')\n",
    "pickle.dump(merge_bin_dict, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （ii）当取值>5时：用bad_rate进行编码，放入连续型变量里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BadRateEncoding(df, col, target):\n",
    "    '''\n",
    "    :param df: dataframe containing feature and target\n",
    "    :param col: the feature that needs to be encoded with bad rate, usually categorical type\n",
    "    :param target: good/bad indicator\n",
    "    :return: the assigned bad rate to encode the categorical feature\n",
    "    '''\n",
    "    regroup = BinBadRate(df, col, target, grantRateIndicator=0)[1]\n",
    "    # set_index(col) 将col转换为索引\n",
    "    # .to_dict() 根据col索引将bad_rate转换为字典格式：{'col值': {'bad_rate': **}}\n",
    "    # orient='index'参数形成{index -> {column -> value}}的结构,还有‘dict’, ‘list’, ‘series’, ‘split’,‘records’\n",
    "    br_dict = regroup[[col,'bad_rate']].set_index([col]).to_dict(orient='index') \n",
    "    # 转换为{col:bad_rate}格式\n",
    "    for k, v in br_dict.items():\n",
    "        br_dict[k] = v['bad_rate']\n",
    "    badRateEnconding = df[col].map(lambda x: br_dict[x]) # 将col列的值转换为该列的bad_rate\n",
    "    return {'encoding':badRateEnconding, 'bad_rate':br_dict}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换前：\n",
      "{'TN': {'bad_rate': 0.15384615384615385}, 'WY': {'bad_rate': 0.0}, 'FL': {'bad_rate': 0.14308053166536355}, 'NM': {'bad_rate': 0.16470588235294117}, 'IN': {'bad_rate': 0.0}, 'OH': {'bad_rate': 0.09230769230769231}, 'DE': {'bad_rate': 0.06976744186046512}, 'NJ': {'bad_rate': 0.11586901763224182}, 'MT': {'bad_rate': 0.14285714285714285}, 'MI': {'bad_rate': 0.0970873786407767}, 'IL': {'bad_rate': 0.10471976401179942}, 'KY': {'bad_rate': 0.06993006993006994}, 'CA': {'bad_rate': 0.12358490566037736}, 'MD': {'bad_rate': 0.10185185185185185}, 'RI': {'bad_rate': 0.08888888888888889}, 'PA': {'bad_rate': 0.0893939393939394}, 'OK': {'bad_rate': 0.10655737704918032}, 'KS': {'bad_rate': 0.07692307692307693}, 'WA': {'bad_rate': 0.12121212121212122}, 'AR': {'bad_rate': 0.0673076923076923}, 'SC': {'bad_rate': 0.0891089108910891}, 'NH': {'bad_rate': 0.14457831325301204}, 'CT': {'bad_rate': 0.10248447204968944}, 'AK': {'bad_rate': 0.11428571428571428}, 'OR': {'bad_rate': 0.13432835820895522}, 'NV': {'bad_rate': 0.18617021276595744}, 'IA': {'bad_rate': 0.0}, 'TX': {'bad_rate': 0.08481421647819062}, 'SD': {'bad_rate': 0.16}, 'LA': {'bad_rate': 0.08854166666666667}, 'NY': {'bad_rate': 0.0934416715031921}, 'AL': {'bad_rate': 0.058823529411764705}, 'MO': {'bad_rate': 0.13588850174216027}, 'GA': {'bad_rate': 0.15025041736227046}, 'MA': {'bad_rate': 0.10064935064935066}, 'AZ': {'bad_rate': 0.11702127659574468}, 'WI': {'bad_rate': 0.08205128205128205}, 'MS': {'bad_rate': 0.0}, 'NC': {'bad_rate': 0.10891089108910891}, 'NE': {'bad_rate': 0.6666666666666666}, 'ID': {'bad_rate': 0.0}, 'DC': {'bad_rate': 0.06796116504854369}, 'VA': {'bad_rate': 0.0919175911251981}, 'UT': {'bad_rate': 0.09166666666666666}, 'VT': {'bad_rate': 0.08}, 'HI': {'bad_rate': 0.11842105263157894}, 'WV': {'bad_rate': 0.14516129032258066}, 'MN': {'bad_rate': 0.08620689655172414}, 'CO': {'bad_rate': 0.09659090909090909}}\n",
      "转换后：\n",
      "{'TN': 0.15384615384615385, 'WY': 0.0, 'FL': 0.14308053166536355, 'NM': 0.16470588235294117, 'IN': 0.0, 'OH': 0.09230769230769231, 'DE': 0.06976744186046512, 'NJ': 0.11586901763224182, 'MT': 0.14285714285714285, 'MI': 0.0970873786407767, 'IL': 0.10471976401179942, 'KY': 0.06993006993006994, 'CA': 0.12358490566037736, 'MD': 0.10185185185185185, 'RI': 0.08888888888888889, 'PA': 0.0893939393939394, 'OK': 0.10655737704918032, 'KS': 0.07692307692307693, 'WA': 0.12121212121212122, 'AR': 0.0673076923076923, 'SC': 0.0891089108910891, 'NH': 0.14457831325301204, 'CT': 0.10248447204968944, 'AK': 0.11428571428571428, 'OR': 0.13432835820895522, 'NV': 0.18617021276595744, 'IA': 0.0, 'TX': 0.08481421647819062, 'SD': 0.16, 'LA': 0.08854166666666667, 'NY': 0.0934416715031921, 'AL': 0.058823529411764705, 'MO': 0.13588850174216027, 'GA': 0.15025041736227046, 'MA': 0.10064935064935066, 'AZ': 0.11702127659574468, 'WI': 0.08205128205128205, 'MS': 0.0, 'NC': 0.10891089108910891, 'NE': 0.6666666666666666, 'ID': 0.0, 'DC': 0.06796116504854369, 'VA': 0.0919175911251981, 'UT': 0.09166666666666666, 'VT': 0.08, 'HI': 0.11842105263157894, 'WV': 0.14516129032258066, 'MN': 0.08620689655172414, 'CO': 0.09659090909090909}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "#\n",
    "\"\"\"\n",
    "\n",
    "regroup = BinBadRate(trainData, 'addr_state', 'y', grantRateIndicator=0)[1]\n",
    "br_dict = regroup[['addr_state','bad_rate']].set_index(['addr_state']).to_dict(orient='index') \n",
    "print(\"转换前：\")\n",
    "print(br_dict)\n",
    "# 转换为{col:bad_rate}格式\n",
    "for k, v in br_dict.items():\n",
    "    br_dict[k] = v['bad_rate']\n",
    "print(\"转换后：\")\n",
    "print(br_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_encoding_dict = {}  # 记录按照bad rate进行编码的变量，及编码方式\n",
    "for col in more_value_features:\n",
    "    br_encoding = BadRateEncoding(trainData, col, 'y')\n",
    "    trainData[col + '_br_encoding'] = br_encoding['encoding']\n",
    "    br_encoding_dict[col] = br_encoding['bad_rate']\n",
    "    num_features.append(col + '_br_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open(folderOfData + 'br_encoding_dict.pkl', 'wb+')\n",
    "pickle.dump(br_encoding_dict, file2)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （iii）对连续型变量进行分箱，包括（ii）中的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(df, col, numOfSplit, special_attribute=[]):\n",
    "    '''\n",
    "    :param df: 按照col排序后的数据集\n",
    "    :param col: 待分箱的变量\n",
    "    :param numOfSplit: 切分的组别数\n",
    "    :param special_attribute: 在切分数据集的时候，某些特殊值需要排除在外\n",
    "    :return: 在原数据集上增加一列，把原始细粒度的col重新划分成粗粒度的值，便于分箱中的合并处理\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    if special_attribute != []:\n",
    "        df2 = df.loc[~df[col].isin(special_attribute)]\n",
    "    N = df2.shape[0]\n",
    "    n = int(N/numOfSplit)\n",
    "    splitPointIndex = [i*n for i in range(1,numOfSplit)] # 按照numOfSplit为间隔划分\n",
    "    rawValues = sorted(list(df2[col]))\n",
    "    splitPoint = [rawValues[i] for i in splitPointIndex]\n",
    "    splitPoint = sorted(list(set(splitPoint)))\n",
    "    return splitPoint # col中“切分点“右边第一个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08481421647819062,\n",
       " 0.0893939393939394,\n",
       " 0.0934416715031921,\n",
       " 0.09659090909090909,\n",
       " 0.10471976401179942,\n",
       " 0.11586901763224182,\n",
       " 0.12358490566037736,\n",
       " 0.14308053166536355]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "# SplitData\n",
    "\"\"\"\n",
    "SplitData(trainData,'addr_state_br_encoding',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignBin(x, cutOffPoints,special_attribute=[]):\n",
    "    '''\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param cutOffPoints: 上述变量的分箱结果，用切分点表示\n",
    "    :param special_attribute:  不参与分箱的特殊取值\n",
    "    :return: 分箱后的对应的第几个箱，从0开始\n",
    "    for example, if cutOffPoints = [10,20,30], if x = 7, return Bin 0. If x = 35, return Bin 3\n",
    "    '''\n",
    "    numBin = len(cutOffPoints) + 1 + len(special_attribute)\n",
    "    if x in special_attribute:\n",
    "        i = special_attribute.index(x)+1\n",
    "        return 'Bin {}'.format(0-i)\n",
    "    if x<=cutOffPoints[0]:\n",
    "        return 'Bin 0'\n",
    "    elif x > cutOffPoints[-1]:\n",
    "        return 'Bin {}'.format(numBin-1)\n",
    "    else:\n",
    "        for i in range(0,numBin-1):\n",
    "            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:\n",
    "                return 'Bin {}'.format(i+1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chi2(df, total_col, bad_col):\n",
    "    '''\n",
    "    :param df: 包含全部样本总计与坏样本总计的数据框\n",
    "    :param total_col: 全部样本的个数\n",
    "    :param bad_col: 坏样本的个数\n",
    "    :return: 卡方值\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    # 求出df中，总体的坏样本率和好样本率\n",
    "    badRate = sum(df2[bad_col])*1.0/sum(df2[total_col])\n",
    "    # 当全部样本只有好或者坏样本时，卡方值为0\n",
    "    if badRate in [0,1]:\n",
    "        return 0\n",
    "    df2['good'] = df2.apply(lambda x: x[total_col] - x[bad_col], axis = 1)\n",
    "    goodRate = sum(df2['good']) * 1.0 / sum(df2[total_col])\n",
    "    # 期望坏（好）样本个数＝全部样本个数*平均坏（好）样本占比\n",
    "    df2['badExpected'] = df[total_col].apply(lambda x: x*badRate)\n",
    "    df2['goodExpected'] = df[total_col].apply(lambda x: x * goodRate)\n",
    "    badCombined = zip(df2['badExpected'], df2[bad_col])\n",
    "    goodCombined = zip(df2['goodExpected'], df2['good'])\n",
    "    badChi = [(i[0]-i[1])**2/i[0] for i in badCombined]\n",
    "    goodChi = [(i[0] - i[1]) ** 2 / i[0] for i in goodCombined]\n",
    "    chi2 = sum(badChi) + sum(goodChi)\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.44504431847173"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "\"\"\"\n",
    "binBadRate, regroup, overallRate = BinBadRate(trainData, 'addr_state_br_encoding', 'y', grantRateIndicator=1)\n",
    "Chi2(regroup,'total','bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ChiMerge_MaxInterval: split the continuous variable using Chi-square value by specifying the max number of intervals\n",
    "def ChiMerge(df, col, target, max_interval=5,special_attribute=[],minBinPcnt=0):\n",
    "    '''\n",
    "    :param df: 包含目标变量与分箱属性的数据框\n",
    "    :param col: 需要分箱的属性\n",
    "    :param target: 目标变量，取值0或1\n",
    "    :param max_interval: 最大分箱数。如果原始属性的取值个数低于该参数，不执行这段函数\n",
    "    :param special_attribute: 不参与分箱的属性取值\n",
    "    :param minBinPcnt：最小箱的占比，默认为0\n",
    "    :return: 分箱结果\n",
    "    '''\n",
    "    colLevels = sorted(list(set(df[col]))) # col列的值排序\n",
    "    N_distinct = len(colLevels) \n",
    "    if N_distinct <= max_interval:  #如果原始属性的取值个数低于max_interval，不执行这段函数\n",
    "        print (\"The number of original levels for {} is less than or equal to max intervals\".format(col))\n",
    "        return colLevels[:-1]\n",
    "    else:\n",
    "        # 不包含speciaL_attribute\n",
    "        if len(special_attribute)>=1:\n",
    "            df1 = df.loc[df[col].isin(special_attribute)]\n",
    "            df2 = df.loc[~df[col].isin(special_attribute)]\n",
    "        else:\n",
    "            df2 = df.copy() # 去掉special_attribute后的df\n",
    "        N_distinct = len(list(set(df2[col]))) #去除speciaL_attribute之后的元素个数\n",
    "\n",
    "        # 步骤一: 通过col对数据集进行分组，求出每组的总样本数与坏样本数\n",
    "        # 最大切分100个组\n",
    "        if N_distinct > 100:\n",
    "            split_x = SplitData(df2, col, 100)\n",
    "            df2['temp'] = df2[col].map(lambda x: AssignGroup(x, split_x))\n",
    "            # Assgingroup函数：每一行的数值和切分点做对比，返回原值在切分后的映射，\n",
    "            # 经过map以后，生成该特征的值对象的“分箱”后的值\n",
    "        else:\n",
    "            df2['temp'] = df2[col]\n",
    "        # 总体bad rate将被用来计算expected bad count\n",
    "        (binBadRate, regroup, overallRate) = BinBadRate(df2, 'temp', target, grantRateIndicator=1)\n",
    "\n",
    "        # 首先，每个单独的属性值将被分为单独的一组\n",
    "        # 对属性值进行排序，然后两两组别进行合并\n",
    "        colLevels = sorted(list(set(df2['temp'])))\n",
    "        groupIntervals = [[i] for i in colLevels] #把每个箱的值打包成[[],[]]的形式\n",
    "\n",
    "        # 步骤二：建立循环，不断合并最优的相邻两个组别，直到：\n",
    "        # 1，最终分裂出来的分箱数<＝预设的最大分箱数\n",
    "        # 2，每箱的占比不低于预设值（可选）\n",
    "        # 3，每箱同时包含好坏样本\n",
    "        # 如果有特殊属性，那么最终分裂出来的分箱数＝预设的最大分箱数－特殊属性的个数\n",
    "        split_intervals = max_interval - len(special_attribute)\n",
    "        while (len(groupIntervals) > split_intervals):  # 终止条件: 当前分箱数＝预设的分箱数\n",
    "            # 每次循环时, 计算合并相邻组别后的卡方值。具有最小卡方值的合并方案，是最优方案\n",
    "            chisqList = []\n",
    "            for k in range(len(groupIntervals)-1):\n",
    "                temp_group = groupIntervals[k] + groupIntervals[k+1]\n",
    "                df2b = regroup.loc[regroup['temp'].isin(temp_group)]\n",
    "                #chisq = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq = Chi2(df2b, 'total', 'bad')\n",
    "                chisqList.append(chisq)\n",
    "            best_comnbined = chisqList.index(min(chisqList))\n",
    "            # 把groupIntervals的值改成类似的值改成类似从[[1][2],[3]]到[[1,2],[3]]\n",
    "            groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined+1]\n",
    "            groupIntervals.remove(groupIntervals[best_comnbined+1])\n",
    "        groupIntervals = [sorted(i) for i in groupIntervals]\n",
    "        cutOffPoints = [max(i) for i in groupIntervals[:-1]] #\n",
    "\n",
    "        # 检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本\n",
    "        groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints)) #每个原始箱对应卡方分箱后的箱号\n",
    "        df2['temp_Bin'] = groupedvalues\n",
    "        (binBadRate,regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "        #返回（每箱坏样本率字典，和包含“列名、坏样本数、总样本数、坏样本率的数据框”）\n",
    "        [minBadRate, maxBadRate] = [min(binBadRate.values()),max(binBadRate.values())]\n",
    "        while minBadRate ==0 or maxBadRate == 1:\n",
    "            # 找出全部为好／坏样本的箱\n",
    "            indexForBad01 = regroup[regroup['bad_rate'].isin([0,1])].temp_Bin.tolist()\n",
    "            bin=indexForBad01[0]\n",
    "            # 如果是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除\n",
    "            if bin == max(regroup.temp_Bin):\n",
    "                cutOffPoints = cutOffPoints[:-1]\n",
    "            # 如果是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除\n",
    "            elif bin == min(regroup.temp_Bin):\n",
    "                cutOffPoints = cutOffPoints[1:]\n",
    "            # 如果是中间的某一箱，则需要和前后中的一个箱进行合并，依据是较小的卡方值\n",
    "            else:\n",
    "                # 和前一箱进行合并，并且计算卡方值\n",
    "                currentIndex = list(regroup.temp_Bin).index(bin)\n",
    "                prevIndex = list(regroup.temp_Bin)[currentIndex - 1]\n",
    "                df3 = df2.loc[df2['temp_Bin'].isin([prevIndex, bin])]\n",
    "                (binBadRate, df2b) = BinBadRate(df3, 'temp_Bin', target)\n",
    "                #chisq1 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq1 = Chi2(df2b, 'total', 'bad')\n",
    "                # 和后一箱进行合并，并且计算卡方值\n",
    "                laterIndex = list(regroup.temp_Bin)[currentIndex + 1]\n",
    "                df3b = df2.loc[df2['temp_Bin'].isin([laterIndex, bin])]\n",
    "                (binBadRate, df2b) = BinBadRate(df3b, 'temp_Bin', target)\n",
    "                #chisq2 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq2 = Chi2(df2b, 'total', 'bad')\n",
    "                if chisq1 < chisq2:\n",
    "                    cutOffPoints.remove(cutOffPoints[currentIndex - 1])\n",
    "                else:\n",
    "                    cutOffPoints.remove(cutOffPoints[currentIndex])\n",
    "            # 完成合并之后，需要再次计算新的分箱准则下，每箱是否同时包含好坏样本\n",
    "            groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "            df2['temp_Bin'] = groupedvalues\n",
    "            (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "            [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "        # 需要检查分箱后的最小占比\n",
    "        if minBinPcnt > 0:\n",
    "            groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "            df2['temp_Bin'] = groupedvalues\n",
    "            valueCounts = groupedvalues.value_counts().to_frame()\n",
    "            valueCounts['pcnt'] = valueCounts['temp'].apply(lambda x: x * 1.0 / sum(valueCounts['temp']))\n",
    "            valueCounts = valueCounts.sort_index()\n",
    "            minPcnt = min(valueCounts['pcnt'])\n",
    "            while minPcnt < minBinPcnt and len(cutOffPoints) > 2:\n",
    "                # 找出占比最小的箱\n",
    "                indexForMinPcnt = valueCounts[valueCounts['pcnt'] == minPcnt].index.tolist()[0]\n",
    "                # 如果占比最小的箱是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除\n",
    "                if indexForMinPcnt == max(valueCounts.index):\n",
    "                    cutOffPoints = cutOffPoints[:-1]\n",
    "                # 如果占比最小的箱是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除\n",
    "                elif indexForMinPcnt == min(valueCounts.index):\n",
    "                    cutOffPoints = cutOffPoints[1:]\n",
    "                # 如果占比最小的箱是中间的某一箱，则需要和前后中的一个箱进行合并，依据是较小的卡方值\n",
    "                else:\n",
    "                    # 和前一箱进行合并，并且计算卡方值\n",
    "                    currentIndex = list(valueCounts.index).index(indexForMinPcnt)\n",
    "                    prevIndex = list(valueCounts.index)[currentIndex - 1]\n",
    "                    df3 = df2.loc[df2['temp_Bin'].isin([prevIndex, indexForMinPcnt])]\n",
    "                    (binBadRate, df2b) = BinBadRate(df3, 'temp_Bin', target)\n",
    "                    #chisq1 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                    chisq1 = Chi2(df2b, 'total', 'bad')\n",
    "                    # 和后一箱进行合并，并且计算卡方值\n",
    "                    laterIndex = list(valueCounts.index)[currentIndex + 1]\n",
    "                    df3b = df2.loc[df2['temp_Bin'].isin([laterIndex, indexForMinPcnt])]\n",
    "                    (binBadRate, df2b) = BinBadRate(df3b, 'temp_Bin', target)\n",
    "                    #chisq2 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                    chisq2 = Chi2(df2b, 'total', 'bad')\n",
    "                    if chisq1 < chisq2:\n",
    "                        cutOffPoints.remove(cutOffPoints[currentIndex - 1])\n",
    "                    else:\n",
    "                        cutOffPoints.remove(cutOffPoints[currentIndex])\n",
    "                groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "                df2['temp_Bin'] = groupedvalues\n",
    "                valueCounts = groupedvalues.value_counts().to_frame()\n",
    "                valueCounts['pcnt'] = valueCounts['temp'].apply(lambda x: x * 1.0 / sum(valueCounts['temp']))\n",
    "                valueCounts = valueCounts.sort_index()\n",
    "                minPcnt = min(valueCounts['pcnt'])\n",
    "        cutOffPoints = special_attribute + cutOffPoints\n",
    "        return cutOffPoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06993006993006994,\n",
       " 0.10891089108910891,\n",
       " 0.12358490566037736,\n",
       " 0.18617021276595744]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test\n",
    "\"\"\"\n",
    "ChiMerge(trainData, 'addr_state_br_encoding', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 判断某变量的坏样本率是否单调\n",
    "def BadRateMonotone(df, sortByVar, target,special_attribute = []):\n",
    "    '''\n",
    "    :param df: 包含检验坏样本率的变量，和目标变量\n",
    "    :param sortByVar: 需要检验坏样本率的变量\n",
    "    :param target: 目标变量，0、1表示好、坏\n",
    "    :param special_attribute: 不参与检验的特殊值\n",
    "    :return: 坏样本率单调与否\n",
    "    '''\n",
    "    df2 = df.loc[~df[sortByVar].isin(special_attribute)]\n",
    "    if len(set(df2[sortByVar])) <= 2:\n",
    "        return True\n",
    "    regroup = BinBadRate(df2, sortByVar, target)[1]\n",
    "    combined = zip(regroup['total'],regroup['bad'])\n",
    "    badRate = [x[1]*1.0/x[0] for x in combined]\n",
    "    badRateNotMonotone = [badRate[i]<badRate[i+1] and badRate[i] < badRate[i-1] or badRate[i]>badRate[i+1] and badRate[i] > badRate[i-1]\n",
    "                       for i in range(1,len(badRate)-1)]\n",
    "    if True in badRateNotMonotone:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AssignGroup(x, bin):\n",
    "    '''\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param bin: 上述变量的分箱结果\n",
    "    :return: x在分箱结果下的映射\n",
    "    '''\n",
    "    N = len(bin)\n",
    "    if x<=min(bin):\n",
    "        return min(bin)\n",
    "    elif x>max(bin):\n",
    "        return 10e10\n",
    "    else:\n",
    "        for i in range(N-1):\n",
    "            if bin[i] < x <= bin[i+1]:\n",
    "                return bin[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int_rate_clean is in processing\n",
      "emp_length_clean is in processing\n",
      "annual_inc is in processing\n",
      "dti is in processing\n",
      "delinq_2yrs is in processing\n",
      "earliest_cr_to_app is in processing\n",
      "inq_last_6mths is in processing\n",
      "mths_since_last_record_clean is in processing\n",
      "mths_since_last_delinq_clean is in processing\n",
      "open_acc is in processing\n",
      "pub_rec is in processing\n",
      "The number of original levels for pub_rec is less than or equal to max intervals\n",
      "total_acc is in processing\n",
      "limit_income is in processing\n",
      "earliest_cr_to_app is in processing\n",
      "purpose_br_encoding is in processing\n",
      "zip_code_br_encoding is in processing\n",
      "addr_state_br_encoding is in processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "continous_merged_dict = {}\n",
    "for col in num_features:\n",
    "    print(\"{} is in processing\".format(col))\n",
    "    if -1 not in set(trainData[col]):  # －1会当成特殊值处理。如果没有－1，则所有取值都参与分箱\n",
    "        max_interval = 5  # 分箱后的最多的箱数\n",
    "        cutOff = ChiMerge(trainData, col, 'y', max_interval=max_interval, special_attribute=[],\n",
    "                                             minBinPcnt=0)\n",
    "        trainData[col + '_Bin'] = trainData[col].map(lambda x: AssignBin(x, cutOff, special_attribute=[]))\n",
    "        monotone = BadRateMonotone(trainData, col + '_Bin', 'y')  # 检验分箱后的单调性是否满足\n",
    "        while (not monotone):\n",
    "            # 检验分箱后的单调性是否满足。如果不满足，则缩减分箱的个数。\n",
    "            max_interval -= 1\n",
    "            cutOff = ChiMerge(trainData, col, 'y', max_interval=max_interval, special_attribute=[],\n",
    "                                                 minBinPcnt=0)\n",
    "            trainData[col + '_Bin'] = trainData[col].map(\n",
    "                lambda x: AssignBin(x, cutOff, special_attribute=[]))\n",
    "            if max_interval == 2:\n",
    "                # 当分箱数为2时，必然单调\n",
    "                break\n",
    "            monotone = BadRateMonotone(trainData, col + '_Bin', 'y')\n",
    "        newVar = col + '_Bin'\n",
    "        trainData[newVar] = trainData[col].map(lambda x: AssignBin(x, cutOff, special_attribute=[]))\n",
    "        var_bin_list.append(newVar)\n",
    "    else:\n",
    "        max_interval = 5\n",
    "        # 如果有－1，则除去－1后，其他取值参与分箱\n",
    "        cutOff = ChiMerge(trainData, col, 'y', max_interval=max_interval, special_attribute=[-1], minBinPcnt=0)\n",
    "        trainData[col + '_Bin'] = trainData[col].map(lambda x: AssignBin(x, cutOff, special_attribute=[-1]))\n",
    "        monotone = BadRateMonotone(trainData, col + '_Bin', 'y', ['Bin -1'])\n",
    "        while (not monotone):\n",
    "            max_interval -= 1\n",
    "            # 如果有－1，－1的bad rate不参与单调性检验\n",
    "            cutOff = ChiMerge(trainData, col, 'y', max_interval=max_interval, special_attribute=[-1],\n",
    "                                                 minBinPcnt=0)\n",
    "            trainData[col + '_Bin'] = trainData[col].map(\n",
    "                lambda x: AssignBin(x, cutOff, special_attribute=[-1]))\n",
    "            if max_interval == 3:\n",
    "                # 当分箱数为3-1=2时，必然单调\n",
    "                break\n",
    "            monotone = BadRateMonotone(trainData, col + '_Bin', 'y', ['Bin -1'])\n",
    "        newVar = col + '_Bin'\n",
    "        trainData[newVar] = trainData[col].map(\n",
    "            lambda x: AssignBin(x, cutOff, special_attribute=[-1]))\n",
    "        var_bin_list.append(newVar)\n",
    "    continous_merged_dict[col] = cutOff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 第四步：WOE编码、计算IV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcWOE(df, col, target):\n",
    "    '''\n",
    "    :param df: 包含需要计算WOE的变量和目标变量\n",
    "    :param col: 需要计算WOE、IV的变量，必须是分箱后的变量，或者不需要分箱的类别型变量\n",
    "    :param target: 目标变量，0、1表示好、坏\n",
    "    :return: 返回WOE和IV\n",
    "    '''\n",
    "    total = df.groupby([col])[target].count()\n",
    "    total = pd.DataFrame({'total': total})\n",
    "    bad = df.groupby([col])[target].sum()\n",
    "    bad = pd.DataFrame({'bad': bad})\n",
    "    regroup = total.merge(bad, left_index=True, right_index=True, how='left')\n",
    "    regroup.reset_index(level=0, inplace=True)\n",
    "    N = sum(regroup['total'])\n",
    "    B = sum(regroup['bad'])\n",
    "    regroup['good'] = regroup['total'] - regroup['bad']\n",
    "    G = N - B\n",
    "    regroup['bad_pcnt'] = regroup['bad'].map(lambda x: x*1.0/B)\n",
    "    regroup['good_pcnt'] = regroup['good'].map(lambda x: x * 1.0 / G)\n",
    "    regroup['WOE'] = regroup.apply(lambda x: np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)\n",
    "    WOE_dict = regroup[[col,'WOE']].set_index(col).to_dict(orient='index')\n",
    "    for k, v in WOE_dict.items():\n",
    "        WOE_dict[k] = v['WOE']\n",
    "    IV = regroup.apply(lambda x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)\n",
    "    IV = sum(IV)\n",
    "    return {\"WOE\": WOE_dict, 'IV':IV}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WOE_dict = {}\n",
    "IV_dict = {}\n",
    "# 分箱后的变量进行编码，包括：\n",
    "# 1，初始取值个数小于5，且不需要合并的类别型变量。存放在less_value_features中\n",
    "# 2，初始取值个数小于5，需要合并的类别型变量。合并后新的变量存放在var_bin_list中\n",
    "# 3，初始取值个数超过5，需要合并的类别型变量。合并后新的变量存放在var_bin_list中\n",
    "# 4，连续变量。分箱后新的变量存放在var_bin_list中\n",
    "all_var = var_bin_list + less_value_features\n",
    "for var in all_var:\n",
    "    woe_iv = CalcWOE(trainData, var, 'y')\n",
    "    WOE_dict[var] = woe_iv['WOE']\n",
    "    IV_dict[var] = woe_iv['IV']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4 = open(folderOfData + 'WOE_dict.pkl', 'wb+')\n",
    "pickle.dump(WOE_dict, file4)\n",
    "file4.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20ff1f69e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将变量IV值进行降序排列，方便后续挑选变量\n",
    "IV_dict_sorted = sorted(IV_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "IV_values = [i[1] for i in IV_dict_sorted]\n",
    "IV_name = [i[0] for i in IV_dict_sorted]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title('feature IV')\n",
    "plt.bar(range(len(IV_values)), IV_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
